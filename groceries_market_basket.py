# -*- coding: utf-8 -*-
"""groceries-market-basket.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vh__K4SPdbXRVVL0vEzwE0EgoZPmzwWp
"""



import os
import plotly.express as px
import numpy as np
import pandas as pd
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import seaborn as sns
import operator as op
from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

data = pd.read_csv('/content/Groceries_dataset.csv')
data.head()
#loading the dataset

"""There are three columns = Member_number , Date, Iterm Description

* Member_number = Unique number
* Date  = Date of Transaction
* Iterm Description = product brought on this date

"""

data.info()

"""Member_number and Date columns are not in correct datatype so we will change it in down the line"""

data.columns = ['memberID', 'Date', 'itemName']
data.head()

"""we are renaming the columns so it will be handy for us in future"""

nan_values = data.isna().sum()
nan_values

"""Checking the missing values. As we can see there are no missing values"""

data.Date = pd.to_datetime(data.Date)
data.memberID = data['memberID'].astype('str')
data.info()

"""now they are in correct datatype
* Date as Date taken from pandas
* MemberID as string

"""

Sales_weekly = data.resample('w', on='Date').size()
fig = px.line(data, x=Sales_weekly.index, y=Sales_weekly,
              labels={'y': 'Number of Sales',
                     'x': 'Date'})
fig.update_layout(title_text='Number of Sales Weekly',
                  title_x=0.5, title_font=dict(size=18))
fig.show()

"""from above graph highest sales was done on Aug"""

Unique_customer_weekly = data.resample('w', on='Date').memberID.nunique()
fig = px.line(Unique_customer_weekly, x=Unique_customer_weekly.index, y=Unique_customer_weekly,
              labels={'y': 'Number of Customers'})
fig.update_layout(title_text='Number of Customers Weekly',
                  title_x=0.5, title_font=dict(size=18))
fig.show()

"""On september 7th we can see 183 customers brough  and it was highest throught out the dataset

"""

Sales_per_Customer = Sales_weekly / Unique_customer_weekly
fig = px.line(Sales_per_Customer, x=Sales_per_Customer.index, y=Sales_per_Customer,
              labels={'y': 'Sales per Customer Ratio'})
fig.update_layout(title_text='Sales per Customer Weekly',
                  title_x=0.5, title_font=dict(size=18))
fig.update_yaxes(rangemode="tozero")
fig.show()

"""The average number of sales per customer remained relatively steady at 2.2 until the end of December 2014. However, there was a significant increase in this metric in early 2015, reaching a peak of 3.3 sales per customer in the first week of March. This rise in sales per customer can be attributed to a consistent level of weekly sales coupled with a decline in the number of customers during that period. The drop in customer numbers is evident in the "Number of Customers Weekly" graph following March 2015."""

Frequency_of_items = data.groupby(pd.Grouper(key='itemName')).size().reset_index(name='count')
fig = px.treemap(Frequency_of_items, path=['itemName'], values='count')
fig.update_layout(title_text='Frequency of the Items Sold',
                  title_x=0.5, title_font=dict(size=18)
                  )
fig.update_traces(textinfo="label+value")
fig.show()

"""milk, vegetables, buns are highest sold"""

user_item = data.groupby(pd.Grouper(key='memberID')).size().reset_index(name='count') \
    .sort_values(by='count', ascending=False)
fig = px.bar(user_item.head(25), x='memberID', y='count',
             labels={'y': 'Number of Sales',
                     'count': 'Number of Items Bought'},
             color='count')
fig.update_layout(title_text='Top 20 Customers regarding Number of Items Bought',
                  title_x=0.5, title_font=dict(size=18))
fig.update_traces(marker=dict(line=dict(color='#000000', width=1)))
fig.show()

""" 3180 customers are our the best customer as per the above graph


"""

day = data.groupby(data['Date'].dt.strftime('%A'))['itemName'].count()
fig = px.bar(day, x=day.index, y=day, color=day,
             labels={'y': 'Number of Sales',
                     'Date': 'Week Days'})
fig.update_layout(title_text='Number of Sales per Discrete Week Days',
                  title_x=0.5, title_font=dict(size=18))
fig.update_traces(marker=dict(line=dict(color='#000000', width=1)))
fig.show()

"""Thursdays. Sunday and Wednesday days lo .. maximun sales are happend"""

month = data.groupby(data['Date'].dt.strftime('%m'))['itemName'].count()
fig = px.bar(month, x=month.index, y=month, color=month,
             labels={'y': 'Number of Sales',
                     'Date': 'Months'})
fig.update_layout(title_text='Number of Sales per Discrete Months',
                  title_x=0.5, title_font=dict(size=18))
fig.update_traces(marker=dict(line=dict(color='#000000', width=1)))
fig.show()

month_day = data.groupby(data['Date'].dt.strftime('%d'))['itemName'].count()
fig = px.bar(month_day, x=month_day.index, y=month_day, color=month_day,
             labels={'y': 'Number of Sales',
                     'Date': 'Month Days'})
fig.update_layout(title_text='Number of Sales per Discrete Month Days',
                  title_x=0.5, title_font=dict(size=18))
fig.update_traces(marker=dict(line=dict(color='#000000', width=1)))
fig.show()

"""**Market Basket Analysis**

* Stores use market basket analysis to figure out what people like to buy together.

* They look at a lot of receipts to see what products are often bought together. Then, they use this information to make decisions about how to stock their shelves and what discounts to offer. This can help them sell more products and make more money.
"""

baskets = data.groupby(['memberID', 'itemName'])['itemName'].count().unstack().fillna(0).reset_index()
baskets.head()

"""The customers-items matrix is a table that shows what products each customer has bought.

Each row in the table represents a different customer, and each column represents a different product. The numbers in the table show how many times each customer has bought each product.
"""

baskets['whole milk'].sum()

# Encoding the items that sold more than 1
def one_hot_encoder(k):
    if k <= 0:
        return 0
    if k >= 1:
        return 1

baskets_final = baskets.iloc[:, 1:baskets.shape[1]].applymap(one_hot_encoder)
baskets_final.head()

"""Encoded the products which are sold more than 1 one product"""

# Finding the most frequent items sold together
frequent_itemsets = apriori(baskets_final, min_support=0.025, use_colnames=True, max_len=3).sort_values(by='support')
frequent_itemsets.head(25)

"""As you can see from the results above, the most items that appeared together are butter and shopping bags, and spread cheese"""

rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1).sort_values('lift', ascending=False)
rules = rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']]
rules.head(25)

"""Created association rules for indicating astecedent and consequent items

sausage  = yogurt, rolls/buns

root vegetables, whole milk  =  shopping bags

rolls/buns, soda  =  sausage

butter, whole milk  =  yogurt,

and etc. have strong relationships.

# **RFM Analysis**

* calculate the Recency
    * To last purchase date of each customer
    * To last date for our dataset
* **Calculating Recency** by subtracting (last transaction date of dataset) and (last purchase date of each customer)


**Recency Distribution of the Customers**

    * Visit Frequency
    * Visit Frequency Distribution of the Customers
    
**Monetary**

**RFM Scores**
**Distribution of the RFM Segments**
**Relationship between Visit_Frequency and Recency**
"""

# Lets Start with the calculate the Recency

# Finding last purchase date of each customer
Recency = data.groupby(by='memberID')['Date'].max().reset_index()
Recency.columns = ['memberID', 'LastDate']
Recency.head()

# Finding last date for our dataset
last_date_dataset = Recency['LastDate'].max()
last_date_dataset

# Calculating Recency by subtracting (last transaction date of dataset) and (last purchase date of each customer)
Recency['Recency'] = Recency['LastDate'].apply(lambda x: (last_date_dataset - x).days)
Recency.head()



# Frequency of the customer visits
Frequency = data.drop_duplicates(['Date', 'memberID']).groupby(by=['memberID'])['Date'].count().reset_index()
Frequency.columns = ['memberID', 'Visit_Frequency']
Frequency.head()

fig = px.histogram(Frequency, x='Visit_Frequency', opacity=0.85, marginal='box')
fig.update_traces(marker=dict(line=dict(color='#000000', width=1)))
fig.update_layout(title_text='Visit Frequency Distribution of the Customers',
                  title_x=0.5, title_font=dict(size=20))
fig.show()

"""According to Frequency Historgram of the Customers, we can see that most of the customers are distributed between their 2nd-5th visit to the Groceries Store"""

Monetary = data.groupby(by="memberID")['itemName'].count().reset_index()
Monetary.columns = ['memberID', 'Monetary']
Monetary.head()

# I assumed each item has equal price and price is 10
Monetary['Monetary'] = Monetary['Monetary'] * 10
Monetary.head()

# Monetary Distribution of the Customers


fig = px.histogram(Monetary, x='Monetary', opacity=0.85, marginal='box',
                   labels={'itemName': 'Monetary'})
fig.update_traces(marker=dict(line=dict(color='#000000', width=1)))
fig.update_layout(title_text='Monetary Distribution of the Customers',
                  title_x=0.5, title_font=dict(size=20))
fig.show()

# Combining all scores into one DataFrame
RFM = pd.concat([Recency['memberID'], Recency['Recency'], Frequency['Visit_Frequency'], Monetary['Monetary']], axis=1)
RFM.head()

# 5-5 score = the best customers
RFM['Recency_quartile'] = pd.qcut(RFM['Recency'], 5, [5, 4, 3, 2, 1])
RFM['Frequency_quartile'] = pd.qcut(RFM['Visit_Frequency'], 5, [1, 2, 3, 4, 5])

RFM['RF_Score'] = RFM['Recency_quartile'].astype(str) + RFM['Frequency_quartile'].astype(str)
RFM.head()

# 5-5 score = the best customers
RFM['Recency_quartile'] = pd.qcut(RFM['Recency'], 5, [5, 4, 3, 2, 1])
RFM['Frequency_quartile'] = pd.qcut(RFM['Visit_Frequency'], 5, [1, 2, 3, 4, 5])

RFM['RF_Score'] = RFM['Recency_quartile'].astype(str) + RFM['Frequency_quartile'].astype(str)
RFM.head()

segt_map = {  # Segmentation Map [Ref]
    r'[1-2][1-2]': 'hibernating',
    r'[1-2][3-4]': 'at risk',
    r'[1-2]5': 'can\'t loose',
    r'3[1-2]': 'about to sleep',
    r'33': 'need attention',
    r'[3-4][4-5]': 'loyal customers',
    r'41': 'promising',
    r'51': 'new customers',
    r'[4-5][2-3]': 'potential loyalists',
    r'5[4-5]': 'champions'
}

RFM['RF_Segment'] = RFM['RF_Score'].replace(segt_map, regex=True)
RFM.head()

# Distribution of the RFM Segments

x = RFM.RF_Segment.value_counts()
fig = px.treemap(x, path=[x.index], values=x)
fig.update_layout(title_text='Distribution of the RFM Segments', title_x=0.5,
                  title_font=dict(size=20))
fig.update_traces(textinfo="label+value+percent root")
fig.show()

"""According to our RFM analysis, most of the customers are segmented into hibernating group which is they are not visiting our store often and it passed pretty much time after their last visit. You can find detailed information about these segments in the references part of this notebook"""

# Relationship between Visit_Frequency and Recency

fig = px.scatter(RFM, x="Visit_Frequency", y="Recency", color='RF_Segment',
                 labels={"math score": "Math Score",
                         "writing score": "Writing Score"})
fig.update_layout(title_text='Relationship between Visit_Frequency and Recency',
                  title_x=0.5, title_font=dict(size=20))
fig.show()

"""As you can see the graph above, when the visit frequency is low and the recency is high, customers are most likely to segmented into hibernating segment. In contrast, when they are visiting our store frequently, and their recency is low, they are most likely to segmented into champions segment which is the best segment for all of the customer segments.

# ML Models:

* Apriori and Frequent Pattern (Fp)
* Growth algorithms
* RARM
* ECLAT
* ASPMS
"""

import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from prettytable import PrettyTable
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules, fpgrowth

# Load the dataset
groceries = pd.read_csv("/content/Groceries_dataset.csv")

# Get all the transactions as a list of lists
all_transactions = [transaction[1]['itemDescription'].tolist() for transaction in list(groceries.groupby(['Member_number', 'Date']))]

# First 21st transactions in the transactional dataset
len(all_transactions)

# Look at the 10 first transactions
all_transactions[0:10]

# The following instructions transform the dataset into the required format
trans_encoder = TransactionEncoder() # Instanciate the encoder
trans_encoder_matrix = trans_encoder.fit(all_transactions).transform(all_transactions)
trans_encoder_matrix = pd.DataFrame(trans_encoder_matrix, columns=trans_encoder.columns_)

trans_encoder_matrix.head()

def perform_rule_calculation(transact_items_matrix, rule_type="fpgrowth", min_support=0.001):

    start_time = 0
    total_execution = 0

    if(not rule_type=="fpgrowth"):
        start_time = time.time()
        rule_items = apriori(transact_items_matrix,
                       min_support=min_support,
                       use_colnames=True)
        total_execution = time.time() - start_time
        print("Computed Apriori!")

    else:
        start_time = time.time()
        rule_items = fpgrowth(transact_items_matrix,
                       min_support=min_support,
                       use_colnames=True)
        total_execution = time.time() - start_time
        print("Computed Fp Growth!")

    rule_items['number_of_items'] = rule_items['itemsets'].apply(lambda x: len(x))

    return rule_items, total_execution

def compute_association_rule(rule_matrix, metric="lift", min_thresh=1):

    rules = association_rules(rule_matrix,
                              metric=metric,
                              min_threshold=min_thresh)

    return rules

# Plot Lift Vs Coverage(confidence)
def plot_metrics_relationship(rule_matrix, col1, col2):

    fit = np.polyfit(rule_matrix[col1], rule_matrix[col2], 1)
    fit_funt = np.poly1d(fit)
    plt.plot(rule_matrix[col1], rule_matrix[col2], 'yo', rule_matrix[col1],
    fit_funt(rule_matrix[col1]))
    plt.xlabel(col1)
    plt.ylabel(col2)
    plt.title('{} vs {}'.format(col1, col2))

def compare_time_exec(algo1=list, alg2=list):


    execution_times = [algo1[1], algo2[1]]
    algo_names = (algo1[0], algo2[0])
    y=np.arange(len(algo_names))

    plt.bar(y,execution_times,color=['orange', 'blue'])
    plt.xticks(y,algo_names)
    plt.xlabel('Algorithms')
    plt.ylabel('Time')
    plt.title("Execution Time (seconds) Comparison")
    plt.show()

val = {'name':12}
value = list(val.items())[0]
value

"""**Case n°1: Using Fp Growph Algorithm**"""

fpgrowth_matrix, fp_growth_exec_time = perform_rule_calculation(trans_encoder_matrix) # Run the algorithm
print("Fp Growth execution took: {} seconds".format(fp_growth_exec_time))

fpgrowth_matrix.head()

fpgrowth_matrix.tail()

"""**Lift**"""

fp_growth_rule_lift = compute_association_rule(fpgrowth_matrix)

fp_growth_rule_lift.head()

plot_metrics_relationship(fp_growth_rule_lift, col1='lift', col2='confidence')

"""Confidence"""

fp_growth_rule = compute_association_rule(fpgrowth_matrix, metric="confidence", min_thresh=0.2)
fp_growth_rule.head()

fp_growth_rule





"""Case n°2: Using Apriori Algorithm"""

apriori_matrix, apriori_exec_time = perform_rule_calculation(trans_encoder_matrix, rule_type="apriori")
print("Apriori Execution took: {} seconds".format(apriori_exec_time))

apriori_matrix.head()

apriori_matrix.tail()

"""Lift"""

apriori_rule_lift = compute_association_rule(apriori_matrix)

apriori_rule_lift.head()

plot_metrics_relationship(apriori_rule_lift, col1='lift', col2='confidence')

"""Confidence"""

apripri_rule = compute_association_rule(apriori_matrix, metric="confidence", min_thresh=0.2)
apripri_rule.head()

rules = fp_growth_rule

from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

data = pd.read_csv('/content/Groceries_dataset.csv')


groceries = data

groceries.rename(columns = {'Member_number':'id','itemDescription':'item'}, inplace = True)

groceries['Date']= pd.to_datetime(groceries['Date'])

#Extracting year,month and day
groceries['year'] = groceries['Date'].apply(lambda x : x.year)
groceries['month'] = groceries['Date'].apply(lambda x : x.month)
groceries['day'] = groceries['Date'].apply(lambda x : x.day)
groceries['weekday'] = groceries['Date'].apply(lambda x : x.weekday())

#Rearranging the columns
groceries=groceries[['id', 'Date','year', 'month', 'day','weekday','item']]


temp=groceries.copy()
temp['qty_purchased']=temp['id'].map(groceries['id'].value_counts())
# temp=temp[:5000]


#sparse matrix
basket = (temp.groupby(['id', 'item'])['qty_purchased']
          .sum().unstack().reset_index().fillna(0)
          .set_index('id'))


def encode(x):
    if x <= 0:
        return 0
    if x >= 1:
        return 1


basket_sets = basket.applymap(encode)
basket_sets




frequent_itemsets = apriori(basket_sets, min_support=0.07, use_colnames=True)


rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)


rules['lhs items'] = rules['antecedents'].apply(lambda x:len(x) )
rules[rules['lhs items']>1].sort_values('lift', ascending=False).head()

# Replace frozen sets with strings
rules['antecedents_'] = rules['antecedents'].apply(lambda a: ','.join(list(a)))
rules['consequents_'] = rules['consequents'].apply(lambda a: ','.join(list(a)))

# Transform the DataFrame of rules into a matrix using the lift metric
pivot = rules[rules['lhs items']>1].pivot(index = 'antecedents_',
                    columns = 'consequents_', values= 'lift')


# Replace frozen sets with strings
rules['antecedents_'] = rules['antecedents'].apply(lambda a: ','.join(list(a)))
rules['consequents_'] = rules['consequents'].apply(lambda a: ','.join(list(a)))

# Transform the DataFrame of rules into a matrix using the lift metric
pivot = rules[rules['lhs items']>1].pivot(index = 'antecedents_',
                    columns = 'consequents_', values= 'lift')

# Generate a heatmap with annotations on and the colorbar off
sns.heatmap(pivot, annot = True)
plt.yticks(rotation=0)
plt.xticks(rotation=90)
plt.show()

